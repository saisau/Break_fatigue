"""
5ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import time
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler


def get_sample_data():
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    data = {
        'mean_hold_time': [0.10, 0.11, 0.12, 0.09, 0.08,  # å…ƒæ°—
                           0.18, 0.20, 0.22, 0.19, 0.21,  # ç–²åŠ´
                           0.10, 0.12, 0.11, 0.09, 0.10,  # å…ƒæ°—
                           0.17, 0.19, 0.20, 0.18, 0.22], # ç–²åŠ´
        'mean_flight_time': [0.15, 0.14, 0.16, 0.13, 0.12,  # å…ƒæ°—
                             0.30, 0.32, 0.35, 0.28, 0.33,  # ç–²åŠ´
                             0.14, 0.15, 0.13, 0.12, 0.14,  # å…ƒæ°—
                             0.29, 0.31, 0.34, 0.30, 0.32], # ç–²åŠ´
        'fatigue_score': [1, 1, 2, 1, 1,  # å…ƒæ°— (1-3)
                          4, 5, 5, 4, 5,  # ç–²åŠ´ (4-5)
                          1, 2, 1, 1, 2,  # å…ƒæ°—
                          4, 4, 5, 4, 5]  # ç–²åŠ´
    }
    return pd.DataFrame(data)


def compare_algorithms():
    print("=" * 60)
    print("5ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ")
    print("=" * 60)
    
    df = get_sample_data()
    X = df[['mean_hold_time', 'mean_flight_time']]
    y = (df['fatigue_score'] >= 4).astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    algorithms = {
        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(random_state=42),
        'k-NN': KNeighborsClassifier(n_neighbors=3)
    }
    
    results = []
    
    for name, model in algorithms.items():
        # å­¦ç¿’
        model.fit(X_train_scaled, y_train)
        
        # ç²¾åº¦
        accuracy = model.score(X_test_scaled, y_test)
        
        # æŽ¨è«–é€Ÿåº¦
        X_single = X_test_scaled[:1]
        start = time.perf_counter()
        for _ in range(100):
            model.predict(X_single)
        elapsed = (time.perf_counter() - start) / 100 * 1000  # ms
        
        results.append({
            'Algorithm': name,
            'Accuracy': accuracy,
            'Inference (ms)': elapsed
        })
        
        status = "âœ…" if accuracy >= 0.90 and elapsed < 10 else "âŒ"
        print(f"{status} {name:20} | ç²¾åº¦: {accuracy:.2%} | é€Ÿåº¦: {elapsed:.3f} ms")
    
    print("=" * 60)
    
    # ãƒ™ã‚¹ãƒˆã‚’é¸æŠž
    passed = [r for r in results if r['Accuracy'] >= 0.90 and r['Inference (ms)'] < 10]
    if passed:
        best = max(passed, key=lambda x: x['Accuracy'])
        print(f"\nðŸ† ãƒ™ã‚¹ãƒˆ: {best['Algorithm']}")
        print(f"   ç²¾åº¦: {best['Accuracy']:.2%}")
        print(f"   é€Ÿåº¦: {best['Inference (ms)']:.3f} ms")
    
    return results


if __name__ == "__main__":
    compare_algorithms()

